{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86f2e5d",
   "metadata": {},
   "source": [
    "1. What is a Decision Tree, and how does it work in the context of classification?\n",
    "- A **Decision Tree** is a supervised machine learning algorithm used for both classification and regression, but it is most commonly applied in classification tasks.\n",
    "- It works by splitting the dataset into subsets based on the most significant features, using conditions or rules at each internal node. Each node represents a decision point on a feature, branches represent possible outcomes, and leaf nodes represent the final class labels.\n",
    "- The algorithm recursively divides the data using measures like **Gini Index**, **Entropy**, or **Information Gain**, aiming to create pure subsets where most or all data points belong to a single class.\n",
    "- In classification, the decision tree predicts the class of a new instance by traversing from the root node down to a leaf node, following the decision rules based on the feature values of that instance.\n",
    "- This makes decision trees easy to interpret, as they mimic human decision-making with clear “if-else” rules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e95f1",
   "metadata": {},
   "source": [
    "2. Explain the concepts of Gini Impurity and Entropy as impurity measures.How do they impact the splits in a Decision Tree?\n",
    "- **Gini Impurity** and **Entropy** are measures used in Decision Trees to determine how mixed or impure a node is. \n",
    "- Gini Impurity reflects the probability of incorrectly classifying a randomly chosen sample if it was labeled based on the distribution of classes in that node.\n",
    "- Entropy, on the other hand, measures the level of disorder or uncertainty in the data, with higher values indicating that the classes are more evenly mixed.\n",
    "- In the context of a Decision Tree, these measures guide the algorithm in selecting the best splits: the tree tries to divide the data so that each resulting group is as pure as possible, meaning it contains mostly samples of a single class.\n",
    "- By doing this, the tree becomes more accurate in classification, as the decisions at each node become clearer and less uncertain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447936e4",
   "metadata": {},
   "source": [
    "3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
    "- **Pre-pruning** and **Post-pruning** are techniques used to prevent overfitting in Decision Trees.\n",
    "- Pre-pruning, also called *early stopping*, stops the tree from growing too deep by applying constraints such as limiting the maximum depth, requiring a minimum number of samples for a split, or setting a threshold for impurity reduction.\n",
    "- Its practical advantage is that it makes the model simpler and faster to train, which is useful when working with large datasets.\n",
    "- Post-pruning, on the other hand, allows the tree to grow fully and then prunes back branches that do not improve performance, usually by evaluating accuracy on a validation set.\n",
    "- Its practical advantage is that it produces more accurate and generalized models, since pruning is guided by actual performance rather than fixed rules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da00fef1",
   "metadata": {},
   "source": [
    "4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
    "- **Information Gain** in Decision Trees is a measure of how well a feature separates the data into distinct classes.\n",
    "- It represents the reduction in uncertainty or impurity after splitting the dataset based on a specific feature. In simple terms, it tells us how much \"useful information\" a split provides about the target variable.\n",
    "- A higher information gain means the split creates more homogeneous groups, making classification easier.\n",
    "- It is important because the Decision Tree uses this measure to decide the best feature and threshold for splitting at each node, ensuring that the tree becomes more accurate and efficient in distinguishing between classes as it grows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d1ba79",
   "metadata": {},
   "source": [
    "5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
    "- Decision Trees are widely used in real-world applications such as medical diagnosis, where they help doctors identify diseases based on symptoms, credit scoring and risk assessment in finance, customer segmentation and churn prediction in marketing, fraud detection, and even in recommendation systems.\n",
    "- Their main advantage is that they are easy to understand, interpret, and visualize since they follow simple “if-else” rules that resemble human decision-making.\n",
    "- They also handle both numerical and categorical data without requiring heavy preprocessing.\n",
    "- However, their limitations include a tendency to overfit the data if not properly pruned, sensitivity to small changes in the dataset that can lead to different tree structures, and relatively lower predictive accuracy compared to more advanced models like Random Forests or Gradient Boosted Trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Write a Python program to:\n",
    "# ● Load the Iris Dataset\n",
    "# ● Train a Decision Tree Classifier using the Gini criterion\n",
    "# ● Print the model’s accuracy and feature importances\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data   # features\n",
    "y = iris.target # labels\n",
    "\n",
    "# Split the dataset into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train Decision Tree Classifier using Gini criterion\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", round(accuracy, 4))\n",
    "\n",
    "# Print feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdb036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Write a Python program to:\n",
    "#● Load the Iris Dataset\n",
    "#● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy toa fully-grown tree.\n",
    "\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training (70%) and testing (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Fully grown Decision Tree\n",
    "full_tree = DecisionTreeClassifier(random_state=42)\n",
    "full_tree.fit(X_train, y_train)\n",
    "y_pred_full = full_tree.predict(X_test)\n",
    "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
    "\n",
    "# Decision Tree with max_depth = 3\n",
    "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "pruned_tree.fit(X_train, y_train)\n",
    "y_pred_pruned = pruned_tree.predict(X_test)\n",
    "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
    "\n",
    "# Print results\n",
    "print(\"Accuracy of Fully Grown Tree:\", round(accuracy_full, 4))\n",
    "print(\"Accuracy of Tree with max_depth=3:\", round(a_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8176bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.Write a Python program to:\n",
    "#● Load the Boston Housing Dataset\n",
    "#● Train a Decision Tree Regressor\n",
    "#● Print the Mean Squared Error (MSE) and feature importances\n",
    "\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load California Housing dataset (replacement for deprecated Boston dataset)\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Split dataset into training (70%) and testing (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train Decision Tree Regressor\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", round(mse, 4))\n",
    "\n",
    "# Print feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "for feature, importance in zip(housing.feature_names, regressor.feature_importances_):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24321367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Write a Python program to:\n",
    "#● Load the Iris Dataset\n",
    "#● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
    "#● Print the best parameters and the resulting model accuracy\n",
    "\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training (70%) and testing (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    \"max_depth\": [2, 3, 4, 5, None],\n",
    "    \"min_samples_split\": [2, 3, 4, 5, 6, 10]\n",
    "}\n",
    "\n",
    "# Initialize Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# GridSearchCV for tuning\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross validation\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Model Accuracy w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9349a09",
   "metadata": {},
   "source": [
    "10. Imagine you’re working as a data scientist for a healthcare company that\n",
    "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
    "mixed data types and some missing values.\n",
    "Explain the step-by-step process you would follow to:\n",
    "● Handle the missing values\n",
    "● Encode the categorical features\n",
    "● Train a Decision Tree model\n",
    "● Tune its hyperparameters\n",
    "● Evaluate its performance\n",
    "And describe what business value this model could provide in the real-world\n",
    "setting\n",
    "\n",
    "- To build a disease prediction model, the first step would be to handle missing values carefully since healthcare data is rarely complete.\n",
    "- For numerical features, missing values can be imputed using the mean or median depending on the distribution, while categorical features can be filled with the most frequent category or a new label such as “Unknown.” Once the data is complete, categorical features need to be encoded into numerical form so that the model can process them—techniques like one-hot encoding or label encoding can be applied depending on whether the categories are nominal or ordinal.\n",
    "- After preprocessing, a Decision Tree model can be trained on the dataset, as it naturally handles both numerical and categorical inputs and does not require heavy feature scaling.\n",
    "- To improve its performance, hyperparameters such as maximum depth, minimum samples per split, and criterion (Gini or entropy) can be tuned using GridSearchCV with cross-validation.\n",
    "- Once the best model is selected, its performance should be evaluated on a test set using metrics such as accuracy, precision, recall, and F1-score, since in healthcare minimizing false negatives is often critical.\n",
    "- In a real-world setting, such a model could provide significant business value by enabling early detection of diseases, helping doctors prioritize high-risk patients, reducing diagnostic costs, and supporting data-driven decision-making in patient care.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
